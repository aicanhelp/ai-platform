{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9416d0a3-63fd-4c60-a499-4b0731858e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import trainer\n",
    "from pytorch_lightning.core import datamodule\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9c74a5-d31e-4ee4-88e9-2ca63588d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=100):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "            # split data into train and val sets\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "datamodule = MNISTDataModule(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de31385-e07a-4d1a-b769-1396e425d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, feature_dim=128):\n",
    "        torch.manual_seed(421)\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.eval_loss = []\n",
    "        self.eval_accuracy = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(val_batch)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(acc)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": acc}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(test_batch)\n",
    "        self.log(\"test_accuracy\", acc)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": acc}\n",
    "\n",
    "    def _shared_eval(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ccb9c4-8190-4aae-bdf2-b71a45ce56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import (\n",
    "    LightningTrainer,\n",
    "    LightningConfigBuilder,\n",
    "    LightningCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "def build_lightning_config_from_existing_code(use_gpu):\n",
    "    # Create a config builder to encapsulate all required parameters.\n",
    "    # Note that model instantiation and fitting will occur later in the LightingTrainer,\n",
    "    # rather than in the config builder.\n",
    "    config_builder = LightningConfigBuilder()\n",
    "\n",
    "    # 1. define your model\n",
    "    # model = MNISTClassifier(lr=1e-3, feature_dim=128)\n",
    "    config_builder.module(cls=MNISTClassifier, lr=1e-3, feature_dim=128)\n",
    "\n",
    "    # 2. define a ModelCheckpoint callback\n",
    "    # checkpoint_callback = ModelCheckpoint(\n",
    "    #     monitor=\"val_accuracy\", mode=\"max\", save_top_k=3\n",
    "    # )\n",
    "    config_builder.checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n",
    "\n",
    "    # 3. Define a Lightning trainer\n",
    "    # trainer = pl.Trainer(\n",
    "    #     max_epochs=10,\n",
    "    #     accelerator=\"cpu\",\n",
    "    #     strategy=\"ddp\",\n",
    "    #     log_every_n_steps=100,\n",
    "    #     logger=CSVLogger(\"logs\"),\n",
    "    #     callbacks=[checkpoint_callback],\n",
    "    # )\n",
    "    config_builder.trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
    "        log_every_n_steps=100,\n",
    "        logger=CSVLogger(\"logs\"),\n",
    "    )\n",
    "    # You do not need to provide the checkpoint callback and strategy here,\n",
    "    # since LightningTrainer configures them automatically.\n",
    "    # You can also add any other callbacks into LightningConfigBuilder.trainer().\n",
    "\n",
    "    # 4. Parameters for model fitting\n",
    "    # trainer.fit(model, datamodule=datamodule)\n",
    "    config_builder.fit_params(datamodule=datamodule)\n",
    "\n",
    "    # Finally, compile all the configs into a dictionary for LightningTrainer\n",
    "    lightning_config = config_builder.build()\n",
    "    return lightning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19eee00-9c20-428b-bb53-0b2f21bea2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it to False if you want to run without GPUs\n",
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617e1d46-ea24-49e6-a084-d52ebd441a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_config = build_lightning_config_from_existing_code(use_gpu=use_gpu)\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=4, use_gpu=use_gpu)\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-mnist-example\",\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=3,\n",
    "        checkpoint_score_attribute=\"val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e774b6f9-dc49-49c3-ae8c-94bc9d70ada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-15 22:17:55</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:41.98        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.3/62.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T600)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_accuracy</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_5308e_00000</td><td>TERMINATED</td><td>192.168.3.51:947988</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         35.0375</td><td style=\"text-align: right;\">   0.0855576</td><td style=\"text-align: right;\">      0.970121</td><td style=\"text-align: right;\">  -12.5655</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 22:17:13,677\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TrainTrainable pid=947988)\u001b[0m 2023-06-15 22:17:18,292\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=947988)\u001b[0m 2023-06-15 22:17:18,300\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 2023-06-15 22:17:21,145\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]\n",
      "  1%|          | 65536/9912422 [00:00<00:32, 307691.34it/s]\n",
      "  1%|▏         | 131072/9912422 [00:00<00:31, 308784.40it/s]\n",
      "  2%|▏         | 229376/9912422 [00:00<00:21, 444595.19it/s]\n",
      "  3%|▎         | 327680/9912422 [00:00<00:17, 556664.47it/s]\n",
      "  5%|▍         | 458752/9912422 [00:00<00:12, 752219.46it/s]\n",
      "  7%|▋         | 688128/9912422 [00:00<00:08, 1147171.22it/s]\n",
      " 10%|▉         | 983040/9912422 [00:01<00:05, 1631380.41it/s]\n",
      " 14%|█▍        | 1409024/9912422 [00:01<00:03, 2307035.58it/s]\n",
      " 17%|█▋        | 1671168/9912422 [00:01<00:04, 1683435.17it/s]\n",
      " 33%|███▎      | 3244032/9912422 [00:01<00:01, 4047062.72it/s]\n",
      " 41%|████      | 4063232/9912422 [00:01<00:01, 4120803.20it/s]\n",
      " 45%|████▌     | 4489216/9912422 [00:02<00:01, 3074843.85it/s]\n",
      " 58%|█████▊    | 5701632/9912422 [00:02<00:01, 3754025.08it/s]\n",
      " 61%|██████▏   | 6094848/9912422 [00:02<00:01, 2072572.41it/s]\n",
      " 76%|███████▌  | 7503872/9912422 [00:03<00:00, 3162614.02it/s]\n",
      " 85%|████████▍ | 8388608/9912422 [00:03<00:00, 3905203.58it/s]\n",
      " 94%|█████████▎| 9273344/9912422 [00:03<00:00, 4669790.65it/s]\n",
      "100%|██████████| 9912422/9912422 [00:03<00:00, 2999165.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Extracting /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/train-images-idx3-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 5993256.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Extracting /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/train-labels-idx1-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\n",
      "  4%|▍         | 65536/1648877 [00:00<00:05, 300116.83it/s]\n",
      "  8%|▊         | 131072/1648877 [00:00<00:04, 305341.45it/s]\n",
      " 14%|█▍        | 229376/1648877 [00:00<00:03, 447413.77it/s]\n",
      " 20%|█▉        | 327680/1648877 [00:00<00:02, 569585.39it/s]\n",
      " 30%|██▉       | 491520/1648877 [00:00<00:01, 826915.98it/s]\n",
      " 44%|████▎     | 720896/1648877 [00:00<00:00, 1220933.12it/s]\n",
      " 60%|█████▉    | 983040/1648877 [00:01<00:00, 1593141.76it/s]\n",
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1454161.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Extracting /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/t10k-images-idx3-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 48107395.88it/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Missing logger folder: logs/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m Extracting /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw/t10k-labels-idx1-ubyte.gz to /udata/workspace/github-aican/ai-platform/content/ray-experimental/MNIST/raw\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948066)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m   | Name              | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m ---------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 0 | linear_relu_stack | Sequential         | 101 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 1 | accuracy          | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m ---------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 101 K     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 101 K     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m 0.407     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m /home/modongsong/.pyenv/versions/miniconda3-4.7.12/envs/jupyterlab/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m   warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">   pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  val_accuracy</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_5308e_00000</td><td>train_epoch_end</td><td>2023-06-15_22-17-53</td><td>True  </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">               0</td><td>mds-hp    </td><td style=\"text-align: right;\">                        10</td><td>192.168.3.51</td><td style=\"text-align: right;\">947988</td><td>True               </td><td style=\"text-align: right;\">  1080</td><td style=\"text-align: right;\">             35.0375</td><td style=\"text-align: right;\">           2.03801</td><td style=\"text-align: right;\">       35.0375</td><td style=\"text-align: right;\"> 1686838673</td><td style=\"text-align: right;\">   0.0855576</td><td style=\"text-align: right;\">                  10</td><td>5308e_00000</td><td style=\"text-align: right;\">      0.970121</td><td style=\"text-align: right;\">  -12.5655</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948064)\u001b[0m `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=948067)\u001b[0m Missing logger folder: logs/lightning_logs\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "2023-06-15 22:17:55,655\tINFO tune.py:945 -- Total run time: 41.99 seconds (41.98 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.9701211452484131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.08555756509304047, 'val_accuracy': 0.9701211452484131, 'val_loss': -12.565531730651855, 'epoch': 9, 'step': 1080, 'should_checkpoint': True, 'done': True, 'trial_id': '5308e_00000', 'experiment_tag': '0'},\n",
       "  path='/home/modongsong/ray_results/ptl-mnist-example/LightningTrainer_5308e_00000_0_2023-06-15_22-17-13',\n",
       "  checkpoint=LightningCheckpoint(local_path=/home/modongsong/ray_results/ptl-mnist-example/LightningTrainer_5308e_00000_0_2023-06-15_22-17-13/checkpoint_000009)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(\"Validation Accuracy: \", result.metrics[\"val_accuracy\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3db9a68-f608-411e-b4ce-e7951ba8e53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /udata/workspace/github-aican/ai-platform/content/ray-experimental/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2023-06-15 22:17:57.721926: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-15 22:17:57.987913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 22:17:59.168256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a65f35973d4ea29b57bab609badcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9739999771118164     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9739999771118164    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint: LightningCheckpoint = result.checkpoint\n",
    "best_model: pl.LightningModule = checkpoint.get_model(MNISTClassifier)\n",
    "trainer = pl.Trainer()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "result = trainer.test(best_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc0dadb-9de3-4908-9579-e353da787532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 22:18:08,805\tWARNING torch_predictor.py:57 -- You have `use_gpu` as False but there are 1 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `TorchPredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.974\n"
     ]
    }
   ],
   "source": [
    "from ray.train.lightning import LightningPredictor\n",
    "\n",
    "predictor = LightningPredictor.from_checkpoint(\n",
    "    checkpoint, MNISTClassifier, use_gpu=use_gpu\n",
    ")\n",
    "\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    correct_preds = np.sum(preds == labels)\n",
    "    return correct_preds\n",
    "\n",
    "\n",
    "corrects = total = 0\n",
    "for batch in test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.numpy(), labels.numpy()\n",
    "    logits = predictor.predict(inputs)[\"predictions\"]\n",
    "    total += labels.size\n",
    "    corrects += accuracy(logits, labels)\n",
    "\n",
    "print(\"Accuracy: \", corrects / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d71a8-7422-4bab-baba-6b280acf3ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
