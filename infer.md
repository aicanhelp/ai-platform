1、Tengine
engine重点聚焦边缘（嵌入式）场景，解决DNN模型Inference在边缘设备上的高效执行，
涵盖了DNN模型优化、推理和异构调度计算。
Tengine支持Inference过程涵盖了模型的加载解析，格式转换，计算图的调度和优化，
在多种架构的芯片上高效运行，具有通用，开放，高性能等特点

面向“端、边、云”优化设计的全场景AI基础设施方案，充分满足智能时代多变的AI应用场景。

支持业界主流框架，方便易用的代码迁移和模型转换工具，通过灵活的合作方式与业界ISV共建开放产业生态。

#### 多框架模型统一管理

主要面向企业AI应用部署及在线服务管理场景，
通过统一应用接口、算力弹性伸缩、A/B测试、滚动发布、多模型加权评估等全栈AI能力，
为企业提供可靠、易用、灵活的推理服务部署及计算资源管理平台，帮助用户AI业务快速上线，
提高AI计算资源的利用效率，实现AI产业的快速落地。

#### 应用服务全周期管理
　　支持Tensor Serving、Torch Serving、Triton 推理引擎
　　支持离线测试，方便开发人员在模型发布前验证模型准确性
　　支持A/B测试与滚动发布进行模型迭代升级
　　支持服务弹性伸缩，根据流量自动调整服务实例数量
　　提供周期部署与定时部署的服务调度策略，解决不同时段的业务算力需求
　　支持流量调节、批量处理、业务下线等操作
　　支持边缘场景下的推理服务部署，状态查看及管理操作

#### 计算资源统一管理，敏捷高效调配
　　支持英伟达、昇腾、寒武纪、昆仑芯、高通等6家国际国内厂商的12款AI和GPU芯片的多元算力支持，
针对不同AI应用的计算特点按需动态调配计算资源，支持GPU单卡算力细粒度切分，集群资源利用率从40% 提高到 80%；

#### 模型压缩和加速：
通过TensorRT进行模型结构优化、量化和定点校准、剪枝等实现模型的有效压缩，
有效降低参数冗余，从而减少存储占用、通信带宽和计算负责度，提升模型的推理性能和资源得利用率，加速能力的应用部署。
通过TensorRT在推理层面实施图优化、算子优化、算子融合、内存优化等优化加速手段，有效提升推理服务性能。
为云边协同的混合AI服务提供

#### 模型云边协同部署和更新：
云、边、端协同处理的混合AI是AI的未来，这样才能高效推动AI规模化落地，并发挥其最大潜能。
依赖基于Kubernetes云原生技术，纳管边缘推理设备，实现云边协同部署和更新，实现
Serverless及混合AI能力服务，实现新型多模型编排和推理，为应用提供隐私安全、个性化、低成本、低时延等推理。

#### 异构高性能推理：
依赖基于Kubernetes调度能力，结合Triton，适配了华为昇腾、寒武纪MLU、天数天垓、燧原云燧等国产AI芯片，
实现了异构推理服务集群构。
支持Pytorch、Tensorflow、ONNXRuntime、TensorRT及自定义的API推理框架等多种推理框架，
实现了动态批处理、多模型的并发高性能推理，
并在 Kubernetes 中实现自动部署和管理推理实例，有效组合源于不同框架的模型及推理资源的调度，
针对不同AI应用的计算特点按需动态调配计算资源，实现资源的高效利用。

#### 体系化AI标准部署：
对模型进行原子化封装，结合Triton，实现Serverless AI能力，采用Ensemble和BLS的模型编排技术，
实现了模型推理的有效编排，并通过动态批处理及内存优化， 提升了推理服务的吞吐能力和时延，提升资源利用率。
AI能力的原子化和Serverless化，有效提升了AI能力的部署效率，增加能力应用的的灵活性和效率，
减小了计算资源的调度粒度，提升了计算资源的调度效率和利用率，也提升了服务能力的有效性和可用性。

有效降低参数冗余，减少存储占用、通信带宽和计算复杂度
TensorRT
图优化、
算子优化、
Memory优化与
INT8 Calibration等

triton:
动态批处理、并发模型执行
多模型的支持，Triton为高效的多模型推理带来了新的模型编排功能。该功能作为生产服务运行，按需加载模型，不使用时卸载模型。
它通过在单个GPU服务器上放置尽可能多的模型来有效地分配GPU资源，并帮助将不同框架的模型分组以高效地使用内存.
优化模型配置与模型分析，Triton的Model Analyzer是一种自动评估Triton推理服务器中模型部署配置的工具，
例如批量大小、精度和目标处理器上的并发执行实例。它帮助选择最优配置以满足应用程序服务质量(QoS)约束(延迟、吞吐量和内存需求)，并将查找最优配置所需的时间从几周缩短到几小时。该工具还支持模型集成和多模型分析。
可在 Kubernetes 中自动部署和管理 Triton 服务器实例，并且有助于组合源于不同框架的模型，以便高效利用内存。

Resource Maximization
更大限度地利用资源
可通过租约系统按需加载模型，并在不使用模型时将其卸载；还可以在单个 GPU 服务器上放置尽可能多的模型

简化部署
可在 Kubernetes 中自动部署和管理 Triton 服务器实例，并且有助于组合源于不同框架的模型，以便高效利用内存。

Resource Maximization
更大限度地利用资源
可通过租约系统按需加载模型，并在不使用模型时将其卸载；还可以在单个 GPU 服务器上放置尽可能多的模型。


云边协同：
云端和终端协同处理的混合AI是AI的未来，这样才能高效推动AI规模化落地，并发挥其最大潜能。
仅靠云端处理已难以支持生成式AI的规模化扩展，在云边协同的混合AI部署下，
即使不同终端处理能力不尽相同，但仍能提供相近的体验，同时带来包括成本、能耗、性能时延、隐私与安全、个性化等优势